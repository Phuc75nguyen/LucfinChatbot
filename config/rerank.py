from sentence_transformers import CrossEncoder
import torch

_reranker_model = None

def load_reranker():
    """
    Loads the Cross-Encoder model as a singleton.
    Forces FP16 via model_kwargs to save VRAM on Quadro T1000.
    """
    global _reranker_model
    if _reranker_model is None:
        device_str = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üöÄ Loading Cross-Encoder on device: {device_str.upper()} (FP16 Mode)")
        
        # S·ª¨A L·ªñI T·∫†I ƒê√ÇY: D√πng model_kwargs ƒë·ªÉ truy·ªÅn torch_dtype
        _reranker_model = CrossEncoder(
            'BAAI/bge-reranker-v2-m3', 
            device=device_str,
            # ƒê√¢y l√† c√°ch ch√≠nh x√°c nh·∫•t cho phi√™n b·∫£n m·ªõi
            model_kwargs={"torch_dtype": torch.float16} 
        )
        
        # C·∫•u h√¨nh max_length sau khi kh·ªüi t·∫°o (An to√†n tuy·ªát ƒë·ªëi)
        _reranker_model.max_length = 512 

    return _reranker_model