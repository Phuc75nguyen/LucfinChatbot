import os
from llama_index.core import StorageContext, load_index_from_storage, Settings
from config.embed import load_embed
from config.llm import load_llm

def get_vector_store():
    # 1. ÄÆ°á»ng dáº«n tá»›i DB vá»«a build
    PERSIST_DIR = "./FoodDB"
    
    if not os.path.exists(PERSIST_DIR):
        raise ValueError(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c '{PERSIST_DIR}'. HÃ£y cháº¡y build_index.py trÆ°á»›c!")

    print(f"ğŸ“‚ Äang táº£i Vector Database tá»«: {PERSIST_DIR}")

    # 2. Cáº¥u hÃ¬nh Global (QUAN TRá»ŒNG: Pháº£i khá»›p vá»›i lÃºc build)
    Settings.embed_model = load_embed()
    Settings.llm = load_llm()

    # 3. Load Index
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    index = load_index_from_storage(storage_context)
    
    print("âœ… ÄÃ£ náº¡p Index thÃ nh cÃ´ng!")
    return index