import asyncio
import os
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper
from dotenv import load_dotenv

# Import directly from API to bypass HTTP overhead
from api.end_points import ask_nutrition, NutritionRequest

# 1. Setup Environment
load_dotenv()
api_key = os.getenv("MY_API_KEY")

# 2. Configure Judge (LLM)
# Using a strong model for evaluation
print("‚öñÔ∏è  Configuring Judge (Llama3-70b)...")
judge_llm = ChatGroq(
    model="mixtral-8x7b-32768", 
    api_key=api_key, 
    temperature=0
)
ragas_judge = LangchainLLMWrapper(judge_llm)

# 3. Configure Embeddings
# Using the same embedding model as the RAG system for consistency
print("üß† Configuring Embeddings (AITeamVN)...")
# Note: We use Langchain's HuggingFaceEmbeddings wrapper here
hf_embeddings = HuggingFaceEmbeddings(
    model_name="AITeamVN/Vietnamese_Embedding",
    model_kwargs={'device': 'cpu'}, # Use CPU for eval to avoid VRAM conflict if needed, or 'cuda'
    encode_kwargs={'normalize_embeddings': False}
)
ragas_embeddings = LangchainEmbeddingsWrapper(hf_embeddings)

# 4. Define Test Data (Ground Truth)
# Format: Question + Ground Truth Answer
test_data_samples = [
    {
        "question": "Ph·ªü b√≤ bao nhi√™u calo?",
        "ground_truth": "M·ªôt t√¥ ph·ªü b√≤ trung b√¨nh ch·ª©a kho·∫£ng 300-450 calo, t√πy thu·ªôc v√†o l∆∞·ª£ng b√°nh ph·ªü, th·ªãt v√† n∆∞·ªõc d√πng."
    },
    {
        "question": "C√°ch n·∫•u canh chua c√° l√≥c mi·ªÅn T√¢y?",
        "ground_truth": "N·∫•u canh chua c√° l√≥c mi·ªÅn T√¢y c·∫ßn c√° l√≥c, b·∫°c h√†, ƒë·∫≠u b·∫Øp, th∆°m, c√† chua, gi√° ƒë·ªó, me chua v√† rau n√™m (ng√≤ gai, rau om). C√° l√†m s·∫°ch, n·∫•u n∆∞·ªõc me, phi t·ªèi, cho c√° v√†o n·∫•u ch√≠n, v·ªõt ra. N·∫•u rau c·ªß, n√™m gia v·ªã chua ng·ªçt, cho c√° l·∫°i, th√™m rau n√™m."
    },
    {
        "question": "ƒÇn chu·ªëi c√≥ b√©o kh√¥ng?",
        "ground_truth": "Chu·ªëi kh√¥ng g√¢y b√©o n·∫øu ƒÉn v·ª´a ph·∫£i. M·ªôt qu·∫£ chu·ªëi trung b√¨nh ch·ª©a kho·∫£ng 105 calo, gi√†u ch·∫•t x∆° v√† kali, t·ªët cho ti√™u h√≥a v√† tim m·∫°ch."
    },
    {
        "question": "B·ªánh ti·ªÉu ƒë∆∞·ªùng n√™n ƒÉn g√¨?",
        "ground_truth": "Ng∆∞·ªùi b·ªánh ti·ªÉu ƒë∆∞·ªùng n√™n ƒÉn rau xanh, ng≈© c·ªëc nguy√™n h·∫°t, c√°c lo·∫°i ƒë·∫≠u, c√° b√©o, th·ªãt n·∫°c. H·∫°n ch·∫ø tinh b·ªôt nhanh, ƒë∆∞·ªùng, ƒë·ªì ng·ªçt v√† ch·∫•t b√©o b√£o h√≤a."
    },
    {
        "question": "100g ·ª©c g√† bao nhi√™u protein?",
        "ground_truth": "100g ·ª©c g√† s·ªëng ch·ª©a kho·∫£ng 23g protein. Khi n·∫•u ch√≠n, l∆∞·ª£ng protein c√≥ th·ªÉ cao h∆°n m·ªôt ch√∫t do m·∫•t n∆∞·ªõc, kho·∫£ng 31g protein."
    }
]

async def run_eval():
    print("üöÄ Starting Evaluation Loop...")
    
    questions = []
    answers = []
    contexts = []
    ground_truths = []
    
    # 5. Data Collection Loop
    for item in test_data_samples:
        q = item["question"]
        gt = item["ground_truth"]
        
        print(f"Testing: {q}")
        
        # Call API function directly
        req = NutritionRequest(question=q, session_id="eval_run")
        response = await ask_nutrition(req)
        
        # Collect data
        questions.append(q)
        answers.append(response.answer)
        # Ragas expects a list of strings for contexts
        # We use the retrieved_contexts field we added to the API
        ctx = response.retrieved_contexts if response.retrieved_contexts else []
        contexts.append(ctx)
        ground_truths.append(gt)
        
        # Optional: Sleep to avoid rate limits if needed
        await asyncio.sleep(1)

    # 6. Prepare Dataset
    data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }
    dataset = Dataset.from_dict(data)
    
    print("üìä Running Ragas Evaluation...")
    
    # 7. Run Evaluation
    results = evaluate(
        dataset=dataset,
        metrics=[
            faithfulness,
            answer_relevancy,
            context_precision,
        ],
        llm=ragas_judge,
        embeddings=ragas_embeddings
    )
    
    print("‚úÖ Evaluation Complete!")
    print(results)
    
    # 8. Export Results
    df = results.to_pandas()
    output_file = "evaluation/scientific_report.xlsx"
    df.to_excel(output_file, index=False)
    print(f"üíæ Report saved to: {output_file}")

if __name__ == "__main__":
    # Run async loop
    asyncio.run(run_eval())
