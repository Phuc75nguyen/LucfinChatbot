import time
from typing import List, Any
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain_core.documents import Document
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

class LlamaIndexRetrieverWrapper(BaseRetriever):
    """Wraps a LlamaIndex retriever to work with LangChain."""
    index: Any
    
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """Get documents relevant to a query."""
        from config.rerank import load_reranker

        start_time = time.time()
        print(f"\nğŸ” [RAG] Báº¯t Ä‘áº§u tÃ¬m kiáº¿m cho: '{query}'")

        # Step 1: Retrieve (High Recall) - Láº¥y 10 á»©ng viÃªn Ä‘á»ƒ trÃ¡nh sÃ³t
        t1 = time.time()
        retriever = self.index.as_retriever(similarity_top_k=5) 
        response = retriever.retrieve(query)
        print(f"â±ï¸  Retrieve Time (LlamaIndex): {time.time() - t1:.4f}s")
        
        # Step 2: Re-rank (Cross-Encoder)
        reranker = load_reranker()
        
        if not response:
            return []
            
        # Prepare pairs: (query, node_text)
        # Chá»‰ láº¥y text thuáº§n tÃºy Ä‘á»ƒ re-rank, trÃ¡nh nhiá»…u metadata
        pairs = [(query, node.get_content()) for node in response]
        
        # --- Tá»I Æ¯U HIá»†U NÄ‚NG GPU (CRITICAL OPTIMIZATION) ---
        t2 = time.time()
        # max_length=512: Cáº¯t ngáº¯n vÄƒn báº£n, giÃºp T1000 khÃ´ng pháº£i tÃ­nh toÃ¡n ma tráº­n quÃ¡ lá»›n
        # batch_size=10: Xá»­ lÃ½ gá»n trong 1 batch
        scores = reranker.predict(
            pairs, 
            batch_size=10,
            show_progress_bar=False
        )
        print(f"â±ï¸  Re-rank Time (GPU): {time.time() - t2:.4f}s")
        
        # Zip nodes with scores and sort by score (descending)
        scored_nodes = list(zip(response, scores))
        scored_nodes.sort(key=lambda x: x[1], reverse=True)
        
        # Pick Top 2 (Precision)
        top_2_nodes = scored_nodes[:2]
        
        # Convert LlamaIndex nodes to LangChain documents
        documents = []
        for node, score in top_2_nodes:
            content = node.get_content()
            metadata = node.metadata.copy() # Copy Ä‘á»ƒ an toÃ n
            # Add score to metadata for debugging
            metadata['re_rank_score'] = float(score)
            documents.append(Document(page_content=content, metadata=metadata))
            
        print(f"ğŸš€ Tá»•ng thá»i gian Pipeline: {time.time() - start_time:.4f}s")
        return documents

def get_conversational_rag_chain(llm, index):
    """
    Creates a conversational RAG chain using LangChain.
    """
    
    # 1. Define the Retriever
    retriever = LlamaIndexRetrieverWrapper(index=index)
    
    # 2. Contextualize Question Prompt
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ]
    )
    
    # Create the history-aware retriever
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    
    # 3. Answer Question Prompt
    # Tá»I Æ¯U SYSTEM PROMPT: Bá» yÃªu cáº§u chÃ o há»i rÆ°á»m rÃ , táº­p trung cáº£nh bÃ¡o y táº¿
    """qa_system_prompt = (
        "Báº¡n lÃ  Lucfin, chuyÃªn gia dinh dÆ°á»¡ng sÃºc tÃ­ch. "
        "Sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n ngá»¯ cáº£nh (Context) dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. "
        "QUY Táº®C Báº®T BUá»˜C: "
        "1. Tráº£ lá»i Ngáº¯n gá»n (dÆ°á»›i 4 dÃ²ng), gáº¡ch Ä‘áº§u dÃ²ng."
        "2. Náº¿u Context khÃ´ng cÃ³ thÃ´ng tin, nÃ³i 'TÃ´i chÆ°a cÃ³ dá»¯ liá»‡u vá» mÃ³n nÃ y'."
        "3. Cáº¢NH BÃO Sá»¨C KHá»E NGHIÃŠM TÃšC náº¿u ngÆ°á»i dÃ¹ng há»i vá» bá»‡nh (tiá»ƒu Ä‘Æ°á»ng, v.v.). "
        "4. KhÃ´ng chÃ o há»i xÃ£ giao (nhÆ° 'ChÃ o báº¡n', 'Ráº¥t vui'). Äi tháº³ng vÃ o váº¥n Ä‘á».\n\n"
        "{context}"
    )"""
    # Tá»I Æ¯U SYSTEM PROMPT (PhiÃªn báº£n V3 - ThÃ­ch á»©ng):
    # - Há»i bá»‡nh/tÆ° váº¥n: Tráº£ lá»i ngáº¯n gá»n, cáº£nh bÃ¡o.
    # - Há»i cÃ´ng thá»©c: Tráº£ lá»i CHI TIáº¾T Ä‘á»‹nh lÆ°á»£ng.
    # Tá»I Æ¯U SYSTEM PROMPT (PhiÃªn báº£n V5 - Cháº·n Ä‘á»©ng áº£o giÃ¡c/Bá»‹a Ä‘áº·t)
    qa_system_prompt = (
        "Báº¡n lÃ  Lucfin, trá»£ lÃ½ dinh dÆ°á»¡ng vÃ  áº©m thá»±c chuyÃªn sÃ¢u cá»§a dá»± Ã¡n NutiAI. "
        "Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i dá»±a trÃªn Dá»¯ liá»‡u (Context) Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i.\n\n"
        
        "QUY Táº®C Xá»¬ LÃ KHI KHÃ”NG CÃ“ Dá»® LIá»†U (Æ¯U TIÃŠN Sá» 1 - Báº®T BUá»˜C):"
        "1. Äá»c ká»¹ Context. Náº¿u tÃªn mÃ³n Äƒn ngÆ°á»i dÃ¹ng há»i KHÃ”NG xuáº¥t hiá»‡n hoáº·c KHÃ”NG liÃªn quan Ä‘áº¿n Context:"
        "   - PHáº¢I TRáº¢ Lá»œI DUY NHáº¤T CÃ‚U SAU: 'MÃ³n \"{input}\" hiá»‡n khÃ´ng cÃ³ trong dá»¯ liá»‡u áº©m thá»±c cá»§a Lucfin. CÃ³ thá»ƒ báº¡n Ä‘ang nháº§m láº«n tÃªn mÃ³n hoáº·c thuáº­t ngá»¯.'"
        "   - TUYá»†T Äá»I KHÃ”NG tá»± phÃ¢n tÃ­ch tá»« ngá»¯ (vÃ­ dá»¥: khÃ´ng Ä‘Æ°á»£c suy diá»…n 'Ä‘Ã¡' lÃ  'tháº¡ch', 'sáº¯t' lÃ  'thá»‹t')."
        "   - TUYá»†T Äá»I KHÃ”NG tá»± bá»‹a ra cÃ´ng thá»©c hoáº·c gá»£i Ã½ mÃ³n thay tháº¿.\n\n"

        "QUY Táº®C Äá»ŠNH Dáº NG (KHI CÃ“ Dá»® LIá»†U):"
        "1. KHÃ”NG dÃ¹ng báº£ng. Chá»‰ dÃ¹ng gáº¡ch Ä‘áº§u dÃ²ng."
        "2. KHI Há»I CÃ”NG THá»¨C: Liá»‡t kÃª Ä‘áº§y Ä‘á»§ NguyÃªn liá»‡u & Äá»‹nh lÆ°á»£ng (náº¿u cÃ³)."
        "3. KHI Há»I Sá»¨C KHá»E: Tráº£ lá»i ngáº¯n gá»n, cáº£nh bÃ¡o bá»‡nh lÃ½."
        "4. ChÃ o há»i xÃ£ giao ngáº¯n gá»n. Äi tháº³ng vÃ o váº¥n Ä‘á».\n\n"
        
        "Dá»® LIá»†U Äáº¦U VÃ€O (Context):\n"
        "{context}"
    )
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
        ]
    )
    
    # Create the document chain
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    # 4. Create the final Retrieval Chain
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    return rag_chain